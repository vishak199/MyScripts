#!/opt/vertica/oss/python3/bin/python3

import argparse
import csv
import datetime
import logging
import os
from pprint import pprint

import traceback
import sys

import vertica_python


########################### GLOBAL VARS ###########################

ARGS = None

CONNECTION = None
CURSOR = None

FAILURE_LIST = []
SUCCESS_LIST = []

KEY_TO_UPDATE = {
    "ETLIncident ": "id",
    "ETLRequest": "id",
}

NEW_ROWS_IDENTIFIER = [
    'ADD_RELATION',
    'ADD_ENTITY',    
]

UPDATE_ROW_IDENTIFIER = [
    'UPDATE_ENTITY',     
]

REMOVE_ROW_IDENTIFER = [
    'REMOVE_ENTITY',
    'REMOVE_RELATION'
]

MASTER_TABLES = [
    'ETLIncident ',
    'ETLRequest',
]

###################################################################


def get_args():
    """ Process and Return command line arguments """
    parser = argparse.ArgumentParser(description='Process CSV files and insert to DV')
    parser.add_argument('csv_files_dir', help='Folder Path of CSV Files')
    parser.add_argument('schema', help='schema name')    
    parser.add_argument('db', help='Database name')
    parser.add_argument('db_user', help='DB username')
    parser.add_argument('db_pass', help='DB password')
    parser.add_argument('sync_type', help='Sync Type')
    parser.add_argument('backup_servers', help='list of backup servers')
    return parser.parse_args()


def get_csv_files():
    """
    Returns CSV files in given folder
    """
    try:
        # list files in dir
        files = os.listdir(ARGS.csv_files_dir)
    except FileNotFoundError:
        logging.error(f'No such directory: {ARGS.csv_files_dir}')
        exit(1)
    except Exception as e:
        logging.error(f'An error occurred while listing files from dir: {ARGS.csv_files_dir}')
        logging.error(e)
        logging.error(traceback.format_exc())
        exit(1)
        
    # filter CSV files
    files = list(filter(lambda f: f.endswith('.csv'), files))

    # sort csv files a-z
    files.sort()    

    return files


def get_db_connection():
    conn_info = {
        'host': ARGS.backup_servers.strip().split(',')[0],
        'port': 5433,
        'user': ARGS.db_user,
        'password': ARGS.db_pass,
        'database': ARGS.db,
        # autogenerated session label by default,
        # 'session_label': 'some_label',
        # default throw error on invalid UTF-8 results
        'unicode_error': 'strict',
        # SSL is disabled by default
        'ssl': False,
        # using server-side prepared statements is disabled by default
        'use_prepared_statements': False,
        # connection timeout is not enabled by default
        # 5 seconds timeout for a socket operation (Establishing a TCP connection or read/write operation)
        'connection_timeout': 3600,
        # 'backup_server_node': ['15.120.115.182', '15.120.115.183']
        'backup_server_node': ARGS.backup_servers.strip().split(','),
    }

    connection  = vertica_python.connect(**conn_info)
    return connection


def create_temp_csv(rows, cols, filename):
    """
    create temp csv file without BI Sync header
    """

    # rename filename
    filename = filename.replace('.csv', '_new.csv')
    file_    = os.path.join('/tmp', filename)

    # headers  = list(rows[0].keys())

    with open(file_, 'w', encoding='utf-8', newline='') as fd:
        writer = csv.DictWriter(fd, fieldnames=cols)
        writer.writeheader()
        for row in rows:
            writer.writerow(dict(row))

    return file_


def execute_sql_command(cmd, filename):
    """
    Exceute DB Command
    """

    logging.info(f'executing command: {cmd}')
    try:
        CURSOR.execute(cmd)
        logging.info(f'Result: {CURSOR.fetchone()[0]}')
        CURSOR.execute('commit')
        return True
    except Exception as e:
        logging.error(f'An error occurred while inserting records from csv {filename}')
        logging.error(traceback.format_exc())        
        FAILURE_LIST.append(filename)
        # exit(1)


def delete_trans_records(trans_table, filename):
    """
    delete records from transaction table
    """
    logging.info(f'deleting records from table {trans_table}')
    cmd = f'delete from {ARGS.schema}.{trans_table}'
    return execute_sql_command(cmd, filename)


def delete_records(table, filename, rows):
    """
    delete records from transaction table
    """

    # check for records
    if len(rows) > 0:
        logging.info(f'deleting records from table {table}')
        # columns = list(map(lambda x: x.lower(), rows[0].keys()))

        unique_id = KEY_TO_UPDATE[table]

        if type(unique_id) == list:            

            condition = []
                
            for row in rows:
                row_lower = {k.lower():v for k, v in row.items()}
                temp = []

                for rec_id in unique_id:
                    temp.append(f"{rec_id}='{row_lower[rec_id]}'")

                condition.append(f'({" AND ".join(temp)})')
                
            where_cond = ' OR '.join(condition)

            cmd = f'DELETE FROM {ARGS.schema}.{table} WHERE {where_cond}'            

        else:                    
            values = []

            # iterate each row to get values
            for row in rows:
                row_lower = {k.lower():v for k, v in row.items()}
                values.append(f"'{row_lower[unique_id]}'")

            unique_ids = ','.join(values)

            cmd = f'DELETE FROM {ARGS.schema}.{table} WHERE {unique_id} IN ({unique_ids})'        
        
        return execute_sql_command(cmd, filename)


def insert_records(table, columns, csv_file, filename):
    """
    insert records to table
    """
    logging.info(f'inserting records to table {table}')

    # construct COPY command to insert data
    columns = ','.join(columns)
    cmd = f"COPY {ARGS.schema}.{table}({columns}) FROM '{csv_file}' PARSER fcsvparser(type='traditional', delimiter=',')"
    return execute_sql_command(cmd, filename)


def merge_data(table, trans_table, filename, columns):
    """
    Executes merge command
    """
    logging.info(f'merging table data: {trans_table} -> {table}')

    # condtruct matching condtion
    unique_id = KEY_TO_UPDATE[table]

    if type(unique_id) == str:
        matching_cond = f'SRC.{unique_id}=TGT.{unique_id}'
    elif type(unique_id) == list:
        matching_cond = ' AND '.join(f'SRC.{col}=TGT.{col}' for col in unique_id)

    # construct cols to update & insert
    cols_to_update = ','.join(f'{col}=SRC.{col}' for col in columns)
    cols_to_insert = ','.join(columns)

    # construct insertion command
    insert_data = 'SRC.' + ',SRC.'.join(columns)

    # construct merge command
    cmd = f"MERGE INTO {ARGS.schema}.{table} TGT USING {ARGS.schema}.{trans_table} SRC ON {matching_cond} WHEN MATCHED THEN UPDATE SET {cols_to_update} WHEN NOT MATCHED THEN INSERT ({cols_to_insert}) VALUES ({insert_data})"
    return execute_sql_command(cmd, filename)


def identify_record_type(rows):
    """
    Identify the records type
    """
    new_and_update_rows = []
    # update_rows = []
    delete_rows = []

    for row in rows:
        if 'BiSyncOperation' in row:
            if row['BiSyncOperation'] in NEW_ROWS_IDENTIFIER:
                new_and_update_rows.append(row)
            elif row['BiSyncOperation'] in UPDATE_ROW_IDENTIFIER:
                new_and_update_rows.append(row)
            elif row['BiSyncOperation'] in REMOVE_ROW_IDENTIFER:
                delete_rows.append(row)
        else:
            new_and_update_rows.append(row)
    return new_and_update_rows, delete_rows


def read_csv(file_):
    """ Reads CSV data and convert to dict"""

    with open(file_, encoding='utf-8') as fd:
        csvreader = csv.DictReader(fd)
        headers = [col.lower() for col in csvreader.fieldnames]
        rows = [row for row in csvreader]

    return rows


def remove_bisync_cols(row):
    if 'RowId' in row:
        row.pop('RowId')
    if 'BiSyncOperation' in row:
        row.pop('BiSyncOperation')
    return row


def get_records_count(table):
    """
    Executes merge command
    """
    
    # construct sql query
    qry = f"SELECT COUNT(*) FROM {ARGS.schema}.{table}"
    
    try:
        CURSOR.execute(qry)
        return CURSOR.fetchone()[0]        
    except Exception as e:
        logging.error(f'An error occurred while executing query: {qry}')
        logging.error(traceback.format_exc())
        FAILURE_LIST.append(filename)        



def sync_data(csv_file, table):
    """
    Sync CSV file data to vertica table
    """

    logging.info(f'{"#" * 15}  {table}  {"#" * 15}')
    logging.info(f'Reading CSV: {csv_file}')

    # read csv data
    rows = read_csv(csv_file)    
    logging.info(f'{len(rows)} record(s) found')

    # extract filename
    filename = os.path.basename(csv_file)


    if len(rows) > 0:

        # get records count before sync
        records_count_before_sync = get_records_count(table)
        # rows = verify_columns(rows, table, filename)        

        # identify the record types
        new_and_update_rows, delete_rows = identify_record_type(rows)
        # print(new_and_update_rows)
        
        logging.info(f'{len(new_and_update_rows)} new & updated entries found')        
        logging.info(f'{len(delete_rows)} removable entries found')

        if table not in TRANSACTION_TABLES:
            # remove BI sync header
            logging.info(f'Removing BI Sync Headers')
            # rows = list(map(remove_bisync_cols, rows))
            new_and_update_rows = list(map(remove_bisync_cols, new_and_update_rows))            
            delete_rows = list(map(remove_bisync_cols, delete_rows))
        
        # delete rows        
        delete_records(table, filename, delete_rows)        
        

        # check for new and updated records
        if len(new_and_update_rows) > 0:

            # verify the columns
            new_and_update_rows = verify_columns(new_and_update_rows, table, filename)

            columns = list(new_and_update_rows[0].keys())
            columns_lower = [col.lower() for col in columns]

            # create temp csv file
            temp_csv_file = create_temp_csv(new_and_update_rows, columns, filename)

            # check table type
            if table in MASTER_TABLES:
                # get transaction table name
                trans_table = f'{table}__transaction' 

                # delete if any existing records from transaction table
                if not delete_trans_records(trans_table, filename):
                    return

                # insert record to transaction table
                if not insert_records(trans_table, columns_lower, temp_csv_file, filename):
                    return

                # merge tables
                if not merge_data(table, trans_table, filename, columns_lower):
                    return

                SUCCESS_LIST.append(filename)

            elif table in RELATIONSHIP_TABLES or table in TRANSACTION_TABLES:
                if not insert_records(table, columns, temp_csv_file, filename):
                    return
                SUCCESS_LIST.append(filename)

            logging.info(f'Number of records in table before sync: {records_count_before_sync}')
            logging.info(f'Number of records in table after sync:  {get_records_count(table)}')


            logging.info(f'Deleting temp csv_file: {temp_csv_file}')
            os.remove(temp_csv_file)
    else:        
        SUCCESS_LIST.append(filename)


def initial_sync(csv_file, table):
    """
    Performs initial sync
    """    

    # logging.info(f'{"#" * 15}  {table}  {"#" * 15}')
    logging.info(f'Identified Table {table}')
    logging.info(f'Reading CSV: {csv_file}')

    # read csv data
    rows = read_csv(csv_file)    
    logging.info(f'{len(rows)} record(s) found')

    if len(rows) > 0:

        # extract filename
        filename = os.path.basename(csv_file)
        if table not in TRANSACTION_TABLES:
            logging.info(f'Removing BI Sync Headers')
            rows = list(map(remove_bisync_cols, rows))

        rows = verify_columns(rows, table, filename)

        columns = list(rows[0].keys())
        columns_lower = [col.lower() for col in columns]

        # create temp csv file
        temp_csv_file = create_temp_csv(rows, columns, filename)

        # insert rows
        if insert_records(table, columns_lower, temp_csv_file, filename):
            SUCCESS_LIST.append(filename)

        logging.info(f'Deleting temp csv_file: {temp_csv_file}')
        os.remove(temp_csv_file)

    else:
        SUCCESS_LIST.append(filename)       


def verify_columns(rows, table, filename):
    """verify the csv columns against table data"""
    cols = get_table_cols(table, filename)
    cols = [col.lower() for col in cols]

    keys = [key.lower() for key in rows[0].keys()]

    logging.info('Verifing columns present in csv file')
    logging.info(f'{len(keys)} columns found in csv and {len(cols)} found in table')    

    if len(cols) == len(keys):
        return rows
    else:
        diff_cols = list(set(cols) - set(keys))
        for row in rows: 
            for col in diff_cols:
                row.update({col: ''})
        
        return rows



def get_table_cols(table, filename):
    """ return list of columns of given table"""
    qry = f"SELECT column_name FROM columns WHERE table_schema = '{ARGS.schema}' AND table_name = '{table}'"

    try:
        CURSOR.execute(qry)
        results = CURSOR.fetchall()
        cols = [row[0] for row in results]        
        # logging.info(f'Result: {cols}')
        return cols
    except Exception as e:
        logging.error(f'An error occurred while executing query: {qry}')
        logging.error(traceback.format_exc())
        FAILURE_LIST.append(filename)
        # exit(1)


def main():
    """
    Main function
    """

    global ARGS, CURSOR, CONNECTION, SUCCESS_LIST, FAILURE_LIST

    # configure log params
    logging.basicConfig(
        # filename='/home/dbadmin/logs/db_sync.log',
        # filemode='a',
        level=logging.DEBUG,
        format="%(asctime)s:%(levelname)s:%(message)s"
    )

    # get command line args
    ARGS = get_args()

    # print(ARGS)

    # set csv file size to sys max
    csv.field_size_limit(sys.maxsize)

    # get csv files
    files = get_csv_files()

    logging.info(f'Total {len(files)} csv files found in dir {ARGS.csv_files_dir}')

    # if no csv files found
    if len(files) == 0:
        exit(0)

    logging.info('Initiating DB connection')
    CONNECTION  = get_db_connection()
    CURSOR      = CONNECTION.cursor()

    all_tables = MASTER_TABLES + TRANSACTION_TABLES + RELATIONSHIP_TABLES

    for file_ in files:        
        logging.info(f'Processing File: {file_}')

        # find table
        _last_index = file_.rfind('_')
        table = file_[:_last_index].lower()

        if 'comment' in table:
            table = 'comments'        
        
        # verify table

        if table is None or table not in all_tables:
            if table is None:
                error = f'unable to find table name from csv file {file_}'
            elif table not in all_tables:
                error = f'Invalid Table {table}, verify the csv file {file_}'

            logging.error(error)
            FAILURE_LIST.append(file_)            

            # continue to next file
            continue

        csv_file = os.path.join(ARGS.csv_files_dir, file_)

        # check sync type
        if ARGS.sync_type.lower() == 'initial' or table == 'comments':
            initial_sync(csv_file, table)
        else:            
            sync_data(csv_file, table)        

    logging.info('Closing DB connection')
    CONNECTION.close()

    logging.info('------SUMMARY-------')    

    logging.info('-----Successfull START----')
    logging.info(f'Successfull CSV list: {len(SUCCESS_LIST)}')

    if len(SUCCESS_LIST) > 0:                
        logging.info('\n'.join(SUCCESS_LIST))

    logging.info('-----Successfull END----')

    logging.info('-----Failure START----')
    logging.info(f'Failure CSV list: {len(FAILURE_LIST)}')
    
    if len(FAILURE_LIST) > 0:                
        logging.info('\n'.join(list(FAILURE_LIST)))
        exit(1)

if __name__ == '__main__':
    main()
